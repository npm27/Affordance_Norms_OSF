####Set up####
##libraries
source(Libraries.R)
####Set up####
##libraries
source("Libraries.R")
##read in data
master = read.csv("Sample.csv", stringsAsFactors = F)
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("Scripts/remove idk.R")
source("Remove idk.R")
View(dat)
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("Remove idk.R")
#remove other weirdness
dat$affordance_response[dat$affordance_response == "unknown"] = NA
dat$affordance_response[dat$affordance_response == "i have no clue"] = NA
dat$affordance_response[dat$affordance_response == "i don't know this word"] = NA
dat$affordance_response[dat$affordance_response == "i do not what this is "] = NA
dat$affordance_response[dat$affordance_response == "i do not know what this is "] = NA
dat$affordance_response[dat$affordance_response == "i dont even know what this is"] = NA
dat$affordance_response[dat$affordance_response == "i do not know"] = NA
##This code is adapted from Buchanan et al. 2019's primer on processing feature production norms
#https://link.springer.com/article/10.1007/s10339-019-00939-6
####Set up####
##libraries
source("Libraries.R")
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("Remove idk.R")
##This code is adapted from Buchanan et al. 2019's primer on processing feature production norms
#https://link.springer.com/article/10.1007/s10339-019-00939-6
####Set up####
##libraries
source("Libraries.R")
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("Remove idk.R")
#remove other weirdness
dat$affordance_response[dat$affordance_response == "unknown"] = NA
dat$affordance_response[dat$affordance_response == "i have no clue"] = NA
dat$affordance_response[dat$affordance_response == "i don't know this word"] = NA
dat$affordance_response[dat$affordance_response == "i do not what this is "] = NA
dat$affordance_response[dat$affordance_response == "i do not know what this is "] = NA
dat$affordance_response[dat$affordance_response == "i dont even know what this is"] = NA
dat$affordance_response[dat$affordance_response == "i do not know"] = NA
View(dat)
#Check for NAs
table(is.na(dat$affordance_response))
#remove nas
dat = na.omit(dat)
####Fix Spelling and Remove White Space####
##Spelling
#Extract a list of words
#tokens = unnest_tokens(tbl = dat, output = token, input = affordance_response)
parsed_afforances = unnest_tokens(tbl = dat, output = parsed,
input = affordance_response, token = "regex",
pattern = ", ")
parsed_afforances = unnest_tokens(tbl = parsed_afforances, output = parsed,
input = parsed, token = "regex",
pattern = ",")
wordlist = unique(parsed_afforances$parsed)
#Run the spell check
spelling.errors = hunspell(wordlist)
spelling.errors = unique(unlist(spelling.errors))
spelling.sugg = hunspell_suggest(spelling.errors, dict = dictionary("en_US"))
#Pick the first spelling suggestion
spelling.sugg = unlist(lapply(spelling.sugg, function(x) x[1]))
#manually check errors
spell_check = cbind(spelling.sugg, spelling.errors)
#Write to file and manually confirm
write.csv(spell_check, file = "spell_check_raw.csv", row.names = F)
#read back in the checked output
spell_check = read.csv("spell_check_raw.csv", stringsAsFactors = F)
spelling.sugg = as.list(spell_check$spelling.sugg)
#get the correct number of observations and make a spelling dictionary
spelling.errors = as.data.frame(spelling.errors)
spelling.dict = (merge(spelling.errors, spell_check, by = 'spelling.errors'))
spelling.dict$spelling.sugg = tolower(spelling.dict$spelling.sugg)
spelling.dict$spelling.pattern = paste0("\\b", spelling.dict$spelling.errors, "\\b")
##Remove white spaces and replace misspellings
parsed_afforances = parsed_afforances[!parsed_afforances$parsed =="", ]
parsed_afforances = unnest_tokens(tbl = parsed_afforances, output = parsed,
input = parsed, token = "regex",
pattern = ",")
parsed_afforances$corrected = stri_replace_all_regex(str = parsed_afforances$parsed,
pattern = spelling.dict$spelling.pattern,
replacement = spelling.dict$spelling.sugg,
vectorize_all = FALSE)
#Fix column names
colnames(parsed_afforances)[6:7] = c("affordance_parsed", "affordance_corrected")
##Write spelled checked data to .csv
write.csv(parsed_afforances, file = "spell_checked.csv", row.names = F)
####Lemmatization####
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
#extract updated tokens
tokens = unnest_tokens(tbl = dat, output = word, input = affordance_corrected)
cuelist = unique(tokens$Stimuli.Cue)
##okay, I think this does what I want.
dat$affordance_lemma = lemmatize_strings(dat$affordance_corrected)
#remove spaces from the middle of words
dat$affordance_corrected = stringr::str_remove_all(dat$affordance_corrected, " ")
##Remove stop words
no_stop = tokens %>%
filter(!grepl("[[:punct:]]", word)) %>% #Remove punctuation
filter(!word %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", word)) %>% #remove numbers
filter(!is.na(word))
temp = data.frame(table(no_stop$word))
####Lemmatize w/ R####
#Lemmatize! (This gives a second set of lemmas using a different algorithm. Also provides part of speech info)
lemmatized = udpipe(no_stop$word, "english")
##Combine datasets and add in second set of Lemmas, part of speech info
combined = cbind(no_stop, lemmatized[ , c(10:11, 13)])
#Give useful column names
colnames(combined)[9] = c("POS")
#Drop unused columns
combined = combined[ , -c(2, 10)]
#don't know why it keeps changing dry to spin-dry though. Going to manually fix that.
combined$lemma[combined$lemma == "spin-dry"] = "dry"
combined$lemma[combined$lemma == "smoothy"] = "smoothie"
##Write to .csv for POS checking
write.csv(combined, file = "Sample Cleaned", row.names = F)
####Set up####
##libraries
source("Libraries.R")
##read in data
master = read.csv("Data/Sample.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("Remove idk.R")
View(dat)
View(dat)
View(dat)
