aoa$Word = tolower(aoa$Word)
##load the n's
n = read.csv("1 Affordance Data/affordance ns.csv")
n$Cue = tolower(n$Cue)
###combine!
##BOI
#merge
combined = merge(cues, BOI, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined = combined[ , -c(2, 4:6)]
colnames(combined)[2] = "BOI"
##concrete
#merge
combined2 = merge(combined, con, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined2 = combined2[ , -c(3, 5:9)]
colnames(combined2)[3] = "Concrete"
##Freq
combined3 = merge(combined2, frq, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined3 = combined3[ , -c(4:8, 10:11)]
colnames(combined3)[4] = "SUBTLEX"
##Add in the n's
combined4 = merge(combined3, n, by.x = "cues",
by.y = "Cue", all.x = TRUE, all.y = FALSE)
##Add in the AOAs
combined5 = merge(combined4, aoa, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
combined5 = combined5[ , -c(8:10, 12:13)]
colnames(combined5)[8] = "AoA"
##now get length
combined5$Length = nchar(combined5$cues)
##ADD AFSS
combined6 = cbind(combined5, AFSS)
##reorder columns (put n last)
combined6 = combined6[ , c(1, 11, 2:4, 8:9, 5:7)]
colnames(combined6)[1] = "Cue"
####Write to .csv####
#write.csv(combined6, file = "Cue Table.csv", row.names = F)
####Write to .csv####
write.csv(combined6, file = "Cue Table.csv", row.names = F)
##load the affordance data
affordances = read.csv("1 Affordance Data/Affordance Norms.csv")
cues = unique(tolower(affordances$cue))
cues = data.frame(cues)
##now load the lexical measures
BOI = read.csv("2 Norm Sets/BOI.csv")
con = read.csv("2 Norm Sets/Concreteness.csv")
frq = read.csv("2 Norm Sets/SUBTLEX-US.csv")
aoa = read.csv("2 Norm Sets/Kuperman AOA.csv")
AFSS = affordances[ , c(1, 4)]
AFSS = unique(AFSS)
BOI$Word = tolower(BOI$Word)
con$Word = tolower(con$Word)
frq$Word = tolower(frq$Word)
aoa$Word = tolower(aoa$Word)
##load the n's
n = read.csv("1 Affordance Data/affordance ns.csv")
n$Cue = tolower(n$Cue)
###combine!
##BOI
#merge
combined = merge(cues, BOI, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
View(combined)
View(BOI)
#drop columns
combined = combined[ , -c(2, 4:6)]
colnames(combined)[2] = "BOI"
##concrete
#merge
combined2 = merge(combined, con, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined2 = combined2[ , -c(3, 5:9)]
colnames(combined2)[3] = "Concrete"
##Freq
combined3 = merge(combined2, frq, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined3 = combined3[ , -c(4:8, 10:11)]
colnames(combined3)[4] = "SUBTLEX"
##Add in the n's
combined4 = merge(combined3, n, by.x = "cues",
by.y = "Cue", all.x = TRUE, all.y = FALSE)
##Add in the AOAs
combined5 = merge(combined4, aoa, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
combined5 = combined5[ , -c(8:10, 12:13)]
colnames(combined5)[8] = "AoA"
##now get length
combined5$Length = nchar(combined5$cues)
##ADD AFSS
combined6 = cbind(combined5, AFSS)
##reorder columns (put n last)
combined6 = combined6[ , c(1, 11, 2:4, 8:9, 5:7)]
colnames(combined6)[1] = "Cue"
View(combined6)
####Set up####
##read in data
affs = read.csv("Data/Affordance Norms_fsg.csv")
cue_table = read.csv("Data/Cue Table.csv")
####This script generate data for the cue table####
##load libraries
library(dplyr)
##load the affordance data
affordances = read.csv("1 Affordance Data/Affordance Norms.csv")
cues = unique(tolower(affordances$cue))
cues = data.frame(cues)
##now load the lexical measures
BOI = read.csv("2 Norm Sets/BOI.csv")
con = read.csv("2 Norm Sets/Concreteness.csv")
frq = read.csv("2 Norm Sets/SUBTLEX-US.csv")
aoa = read.csv("2 Norm Sets/Kuperman AOA.csv")
AFSS = affordances[ , c(1, 4)]
AFSS = unique(AFSS)
BOI$Word = tolower(BOI$Word)
con$Word = tolower(con$Word)
frq$Word = tolower(frq$Word)
aoa$Word = tolower(aoa$Word)
##load the n's
n = read.csv("1 Affordance Data/affordance ns.csv")
n$Cue = tolower(n$Cue)
###combine!
##BOI
#merge
combined = merge(cues, BOI, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined = combined[ , -c(2, 4:6)]
colnames(combined)[2] = "BOI"
##concrete
#merge
combined2 = merge(combined, con, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined2 = combined2[ , -c(3, 5:9)]
colnames(combined2)[3] = "Concrete"
##Freq
combined3 = merge(combined2, frq, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined3 = combined3[ , -c(4:8, 10:11)]
colnames(combined3)[4] = "SUBTLEX"
##Add in the n's
combined4 = merge(combined3, n, by.x = "cues",
by.y = "Cue", all.x = TRUE, all.y = FALSE)
##Add in the AOAs
combined5 = merge(combined4, aoa, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
combined5 = combined5[ , -c(8:10, 12:13)]
colnames(combined5)[8] = "AoA"
##now get length
combined5$Length = nchar(combined5$cues)
##ADD AFSS
combined6 = cbind(combined5, AFSS)
##load the affordance data
affordances = read.csv("1 Affordance Data/Affordance Norms.csv")
cues = unique(tolower(affordances$cue))
cues = data.frame(cues)
##now load the lexical measures
BOI = read.csv("2 Norm Sets/BOI.csv")
con = read.csv("2 Norm Sets/Concreteness.csv")
frq = read.csv("2 Norm Sets/SUBTLEX-US.csv")
aoa = read.csv("2 Norm Sets/Kuperman AOA.csv")
View(affordances)
AFSS = affordances[ , c(1, 5)]
AFSS = unique(AFSS)
BOI$Word = tolower(BOI$Word)
con$Word = tolower(con$Word)
frq$Word = tolower(frq$Word)
aoa$Word = tolower(aoa$Word)
##load the n's
n = read.csv("1 Affordance Data/affordance ns.csv")
n$Cue = tolower(n$Cue)
###combine!
##BOI
#merge
combined = merge(cues, BOI, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
View(combined)
#drop columns
combined = combined[ , -c(2, 4:6)]
colnames(combined)[2] = "BOI"
##concrete
#merge
combined2 = merge(combined, con, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
View(combined2)
#drop columns
combined2 = combined2[ , -c(3, 5:9)]
colnames(combined2)[3] = "Concrete"
##Freq
combined3 = merge(combined2, frq, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
#drop columns
combined3 = combined3[ , -c(4:8, 10:11)]
colnames(combined3)[4] = "SUBTLEX"
##Add in the n's
combined4 = merge(combined3, n, by.x = "cues",
by.y = "Cue", all.x = TRUE, all.y = FALSE)
##Add in the AOAs
combined5 = merge(combined4, aoa, by.x = "cues",
by.y = "Word", all.x = TRUE, all.y = FALSE)
View(combined5)
combined5 = combined5[ , -c(8:10, 12:13)]
colnames(combined5)[8] = "AoA"
##now get length
combined5$Length = nchar(combined5$cues)
##ADD AFSS
combined6 = cbind(combined5, AFSS)
##reorder columns (put n last)
combined6 = combined6[ , c(1, 11, 2:4, 8:9, 5:7)]
colnames(combined6)[1] = "Cue"
View(combined6)
####Write to .csv####
write.csv(combined6, file = "Cue Table.csv", row.names = F)
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Affordance_Norms_OSF/2 Analyses")
dat = read.csv("Data/cue table.csv")
View(dat)
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Affordance-Shiny")
####Merge FSG and Affordances####
afs = read.csv("1 Affordance Data/Affordance Norms.csv")
fsg = read.csv("2 Norm Sets/USF FSG.csv")
sem = read.csv("2 Norm Sets/Buchanan norms.csv")
View(afs)
##fix column names
colnames(afs)[1] = "Cue"
colnames(afs)[2] = "Target"
colnames(fsg)[1] = "Cue"
colnames(fsg)[2] = "Target"
##fix case
fsg$Cue = tolower(fsg$Cue)
fsg$Target = tolower(fsg$Target)
##Now merge!
combined = merge(afs, fsg, by.x = c("Cue", "Target"),
by.y = c("Cue", "Target"), all.x = TRUE, all.y = FALSE)
View(combined)
##Drop BSG and SYM Columns
combined = combined[ , -c(7:8)]
####Add the Buchanan Norms####
colnames(sem)[1] = "Cue"
colnames(sem)[2] = "Target"
combined2 = merge(combined, sem, by.x = c("Cue", "Target"),
by.y = c("Cue", "Target"), all.x = TRUE, all.y = FALSE)
View(combined2)
combined2 = combined2[ , -c(8:14)]
colnames(combined2)[6] = "COS"
View(combined2)
write.csv(combined2[ , -5], file = "Affordance Norms_fsg.csv", row.names = F)
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/Affordance_Norms_OSF/2 Analyses")
####Set up####
##read in data
affs = read.csv("Data/Affordance Norms_fsg.csv")
cue_table = read.csv("Data/Cue Table.csv")
##load libraries
library(psych)
library(dplyr)
##scientific notation
options(scipen = 999)
####Correlations####
##make the subsets
fsg = subset(affs,
is.na(affs$FSG) == F )
cos = subset(affs,
is.na(affs$COS) == F )
###Correlations w/ double word norms
##FSG
cor.test(fsg$AFS, fsg$FSG) #.19
View(fsg)
View(affs)
####Set up####
##read in data
affs = read.csv("Data/Affordance Norms_fsg.csv")
cue_table = read.csv("Data/Cue Table.csv")
##load libraries
library(psych)
library(dplyr)
##scientific notation
options(scipen = 999)
####Correlations####
##make the subsets
fsg = subset(affs,
is.na(affs$FSG) == F )
cos = subset(affs,
is.na(affs$COS) == F )
###Correlations w/ double word norms
##FSG
cor.test(fsg$AFS, fsg$FSG) #.19
cor.test(fsg$AFSS, fsg$FSG) #-.08
View(fsg)
####Set up####
##read in data
affs = read.csv("Data/Affordance Norms_fsg.csv")
cue_table = read.csv("Data/Cue Table.csv")
##load libraries
library(psych)
library(dplyr)
##scientific notation
options(scipen = 999)
####Correlations####
##make the subsets
fsg = subset(affs,
is.na(affs$FSG) == F )
cos = subset(affs,
is.na(affs$COS) == F )
###Correlations w/ double word norms
##FSG
cor.test(fsg$AFS, fsg$FSG) #.18
cor.test(fsg$AFSS, fsg$FSG) #-.08
##COS
cor.test(cos$AFS, cos$COS) #.12
cor.test(cos$AFSS, cos$COS) #-.10
####Set up for Single Word Norms and strongest AFS Pairs####
##get the strongest affordance pairing
temp = data.frame()
cuelist = unique(affs$Cue)
for(i in cuelist){
temp2 = subset(affs,
affs$Cue == i)
temp2 = temp2[order(temp2$AFS, decreasing = TRUE),]
temp2 = temp2[1, ]
temp = rbind(temp2, temp)
}
aff2 = temp
####Strongest AFS Pairs####
##build subsets
fsg2 = subset(aff2,
is.na(aff2$FSG) == F)
cos2 = subset(aff2,
is.na(aff2$COS) == F)
###Semantic Correlations
##FSG
cor.test(fsg2$AFS, fsg2$FSG) #.18
cor.test(fsg2$AFSS, fsg2$FSG) #-.03
##COS
cor.test(cos2$AFS, cos2$COS) #-.09
cor.test(cos2$AFSS, cos2$COS) #-.08
###Lexical Correlations
##strongest affordance
cue_table$Cue = tolower(cue_table$Cue)
aff2$Cue = tolower(aff2$Cue)
combined = merge(aff2, cue_table, by.x = "Cue", by.y = "Cue")
##set size
cor.test(combined$AFSS, combined$BOI) #.13 #BOI
View(combined)
combined = combined[ , -8]
colnames(combined)[5] = "AFSS"
##set size
cor.test(combined$AFSS, combined$BOI) #.13 #BOI
cor.test(combined$AFSS, combined$Concrete) #.09 #CONCRETE
cor.test(combined$AFSS, combined$SUBTLEX) #.30 #SUBTLEX
cor.test(combined$AFSS, combined$AoA) #-.25 #AoA
#Strongest AFS
cor.test(combined$AFS, combined$BOI) #.10 #BOI
cor.test(combined$AFS, combined$Concrete) #.07 #CONCRETE
cor.test(combined$AFS, combined$SUBTLEX) #-.11 #SUBTLEX
cor.test(combined$AFS, combined$AoA) #.01 #AOA #Non-sig
cor.test(combined$AFS, combined$AFSS) #-.44
##Strongest AFP
cor.test(combined$AFP, combined$BOI) #.17 #BOI
cor.test(combined$AFP, combined$Concrete) #.13 #CONCRETE
cor.test(combined$AFP, combined$SUBTLEX) #-.09 #SUBTLEX
cor.test(combined$AFP, combined$AoA) #.01 #AOA #Non-sig
cor.test(combined$AFS, combined$AoA) #.01 #AOA #Non-sig
cor.test(combined$AFP, combined$AFPS) #-.47
cor.test(combined$AFP, combined$AFSS) #-.47
View(combined)
corr.test(combined[ , c(3:5; 8:11)])
corr.test(combined[ , c(3:5, 8:11)])
corr.test(combined[ , c(3:5, 8:11)])
corr.test(combined[ , c(5,3,4, 8:11)])
corr.test(combined[ , c(5,3,4, 9,8,10,11)])
cor.test(combined$AFSS, combined$Concrete) #.01 #CONCRETE
corr.test(combined[ , c(5,3,4, 9,8,10,11)])
dat = read.csv("Data/Affordance Norms_fsg.csv")
View(dat)
fsg = dat[ , -6]
fsg = na.omit(fsg)
cos = dat[ , -5]
cos = na.omit(cos)
nrow(fsg)
nrow(fsg)/nrow(dat)
nrow(cos)/nrow(dat)
#get overlapping pair percentages
nrow(fsg)/nrow * 100
#get overlapping pair percentages
nrow(fsg)/nrow(dat) * 100
nrow(cos)/nrow(dat) * 100
library(psych)
options(scipen = 999)
View(fsg)
corr.test(fsg[ , c(3,4,6)])
corr.test(cos[ , c(3,4,6)])
View(fsg)
View(cos)
##set up
dat = read.csv("Data/Affordance Norms_fsg.csv")
fsg = dat[ , -6]
View(fsg)
##set up
dat = read.csv("Data/Affordance Norms_fsg.csv")
fsg = dat[ , -7]
fsg = na.omit(fsg)
cos = dat[ , -6]
cos = na.omit(cos)
#get overlapping pair percentages
nrow(fsg)/nrow(dat) * 100
cor(fsg$AFS, fsg$FSG)
corr.test(fsg$AFS, fsg$FSG)
corr.p(fsg$AFS, fsg$FSG)
corr.p(fsg$AFS, fsg$FSG)
cor.test(fsg$AFS, fsg$FSG)
cor.test(cos$AFS, cos$COS)
cor.test(fsg$AFP, fsg$FSG)
cor.test(cos$AFP, cos$COS)
cor.test(fsg$AFP, fsg$FSG)
corr.test(fsg[ , c(3,4,6)])
corr.test(cos[ , c(3,4,6)])
temp2 = temp2[1, ]
temp2 = temp2[1, ]
####Set up####
##read in data
affs = read.csv("Data/Affordance Norms_fsg.csv")
cue_table = read.csv("Data/Cue Table.csv")
##load libraries
library(psych)
library(dplyr)
##scientific notation
options(scipen = 999)
####Correlations####
##make the subsets
fsg = subset(affs,
is.na(affs$FSG) == F )
cos = subset(affs,
is.na(affs$COS) == F )
###Correlations w/ double word norms
##FSG
cor.test(fsg$AFS, fsg$FSG) #.18
cor.test(fsg$AFSS, fsg$FSG) #-.08
##COS
cor.test(cos$AFS, cos$COS) #.11
cor.test(cos$AFSS, cos$COS) #-.07
####Set up for Single Word Norms and strongest AFS Pairs####
##get the strongest affordance pairing
temp = data.frame()
cuelist = unique(affs$Cue)
for(i in cuelist){
temp2 = subset(affs,
affs$Cue == i)
temp2 = temp2[order(temp2$AFS, decreasing = TRUE),]
temp2 = temp2[1, ]
temp = rbind(temp2, temp)
}
aff2 = temp
####Strongest AFS Pairs####
##build subsets
fsg2 = subset(aff2,
is.na(aff2$FSG) == F)
cos2 = subset(aff2,
is.na(aff2$COS) == F)
###Semantic Correlations
##FSG
cor.test(fsg2$AFS, fsg2$FSG) #.19
cor.test(fsg2$AFSS, fsg2$FSG) #-.04
##COS
cor.test(cos2$AFS, cos2$COS) #-.10
cor.test(cos2$AFSS, cos2$COS) #-.08
###Lexical Correlations
##strongest affordance
cue_table$Cue = tolower(cue_table$Cue)
aff2$Cue = tolower(aff2$Cue)
combined = merge(aff2, cue_table, by.x = "Cue", by.y = "Cue")
combined = combined[ , -8]
colnames(combined)[5] = "AFSS"
##set size
cor.test(combined$AFSS, combined$BOI) #.11 #BOI
cor.test(combined$AFSS, combined$Concrete) #.01 #CONCRETE
cor.test(combined$AFSS, combined$SUBTLEX) #.33 #SUBTLEX
cor.test(combined$AFSS, combined$AoA) #-.21 #AoA
#Strongest AFS
cor.test(combined$AFS, combined$BOI) #.17 #BOI
cor.test(combined$AFS, combined$Concrete) #.13 #CONCRETE
cor.test(combined$AFS, combined$SUBTLEX) #-.09 #SUBTLEX
cor.test(combined$AFS, combined$AoA) #.01 #AOA #Non-sig
cor.test(combined$AFS, combined$AFSS) #-.47
mean(combined$AFS); sd(combined$AFS)
mean(combined$AFS); sd(combined$AFS)
##Strongest AFP
cor.test(combined$AFP, combined$BOI) #.33 #BOI
cor.test(combined$AFP, combined$Concrete) #.25 #CONCRETE
cor.test(combined$AFP, combined$SUBTLEX) #.08 #SUBTLEX
cor.test(combined$AFP, combined$AoA) #-.21 #AOA #Non-sig
cor.test(combined$AFP, combined$AFSS) #-.09
corr.test(combined[ , c(5,3,4, 9,8,10,11)])
##set up
dat = read.csv("Data/Affordance Norms_fsg.csv")
fsg = dat[ , -7]
fsg = na.omit(fsg)
cos = dat[ , -6]
cos = na.omit(cos)
library(psych)
options(scipen = 999)
#get overlapping pair percentages
nrow(fsg)/nrow(dat) * 100
nrow(cos)/nrow(dat) * 100
corr.test(fsg[ , c(3,4,6)])
corr.test(cos[ , c(3,4,6)])
corr.test(cos[ , c(3,4,6)])
